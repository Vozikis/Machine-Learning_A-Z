{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K4f4JG1gdKqj"
   },
   "source": [
    "#AutoEncoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1jbiqOK7dLGG"
   },
   "source": [
    "##Downloading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XL5MEkLcfRD2"
   },
   "source": [
    "###ML-100K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "id": "rjOPzue7FCXJ",
    "outputId": "b0f0b631-51bf-49e8-9ad6-947a0bd3d832"
   },
   "outputs": [],
   "source": [
    "# !wget \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "# !unzip ml-100k.zip\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Xis6ldDfTs6"
   },
   "source": [
    "###ML-1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "LOly1yfAfTjd",
    "outputId": "519b7fb3-4f15-4a0b-91d5-8221ee486409"
   },
   "outputs": [],
   "source": [
    "# !wget \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
    "# !unzip ml-1m.zip\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOBJ8UCXdY0g"
   },
   "source": [
    "##Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LvGeU1CeCtg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pM04FyMudkoK"
   },
   "source": [
    "## Importing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJw2p3-Cewo4"
   },
   "outputs": [],
   "source": [
    "# We won't be using this dataset.\n",
    "movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTIbE2tkdkwP"
   },
   "source": [
    "## Preparing the training set and the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2usLKJBEgPE2"
   },
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\\t')\n",
    "training_set = np.array(training_set, dtype = 'int')\n",
    "test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\\t')\n",
    "test_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zCf8HjSydk4s"
   },
   "source": [
    "## Getting the number of users and movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gPaGZqdniC5m"
   },
   "outputs": [],
   "source": [
    "nb_users = int(max(max(training_set[:, 0], ), max(test_set[:, 0])))\n",
    "nb_movies = int(max(max(training_set[:, 1], ), max(test_set[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J-w4-hVidlAm"
   },
   "source": [
    "## Converting the data into an array with users in lines and movies in columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-wASs2YFiDaa"
   },
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "  new_data = []\n",
    "  for id_users in range(1, nb_users + 1):\n",
    "    id_movies = data[:, 1] [data[:, 0] == id_users]\n",
    "    id_ratings = data[:, 2] [data[:, 0] == id_users]\n",
    "    ratings = np.zeros(nb_movies)\n",
    "    ratings[id_movies - 1] = id_ratings\n",
    "    new_data.append(list(ratings))\n",
    "  return new_data\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMmhuUpldlHo"
   },
   "source": [
    "## Converting the data into Torch tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwD-KD8yiEEw"
   },
   "outputs": [],
   "source": [
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6kkL8NkkdlZj"
   },
   "source": [
    "## Creating the architecture of the Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oU2nyh76iE6M"
   },
   "outputs": [],
   "source": [
    "#in order to make an autoencoder we need to define a lot of things\n",
    "#first how many layers we want to have\n",
    "#how many nodes in each layer\n",
    "#activation function \n",
    "#optimization function\n",
    "#module contains several classes and a library contains multiple modules\n",
    "# we will take advantage of inheritance of the Pytorch library\n",
    "#we will make a stacked autoencoder\n",
    "#we will take the parent nn.Modules because we want all the variables and modules from the parent class\n",
    "#stacked autoencoder -> many hidden layers -> many encodings\n",
    "#in the parenthesis we will add the parent class\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, ): #we always need to define our init function. No need to add anything else other than\n",
    "        #the self because we will take advantage of the methods and variables of inheritance\n",
    "        #super will get the inheritance methods from parent class\n",
    "        super(SAE, self).__init__() \n",
    "        #full connection between the input and the first hidden layer. Shorter vector than input vector\n",
    "        #we need to use self to specify that fc1 is related to our autoencoders object\n",
    "        #linear is inherited from nn.module. First parameter is number of features. second is the num \n",
    "        #of nodes in the first hidden layer. Based on expirement we choose 20!\n",
    "        #these 20 nodes will represent some features that from unsupervised learning the AE detects\n",
    "        #from the input vector that are liked from simillar people. ex oscar or an actor\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        #second full connection of our hidden layer. neurons of first hidden and 10 the nodes of the second hidden\n",
    "        #it will detect more features but will be based the previous hidden layer\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        #since we are doing deep learning lets add a third hidden layer\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        #output layer with an output layer of the total movies we had in the input. \n",
    "        #in autoencoders we are reconstructing the input vector\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        #activation function. we will define it activation. We need self. We tried rectifier and Sigmoid and\n",
    "        #we got better activation with the latter\n",
    "        #Sigmoid is taken from the parent class. \n",
    "        self.activation = nn.Sigmoid()\n",
    "        #will proceed to the different encodings and ecodings and apply the activation functions inside\n",
    "        #first argument is self. We need to put it every time and we need to use it in order to access our object\n",
    "        #second argument is our input vector.\n",
    "    def forward(self, x):\n",
    "        #self represents our object!\n",
    "        #we are activating the x in each layer \n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        #now we are decoding and not encoding and we dont need to do an activation function since its the last node\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "#create the object of this class. We have to use non capital letters for our object.\n",
    "#since we didn't specify any arguments during our creation process we don't have to do it now.\n",
    "sae = SAE()\n",
    "#Define the criterion we will need to use after\n",
    "#criterion for the loss function. Mean square error. Criterion will be an object of the class.\n",
    "criterion = nn.MSELoss()\n",
    "#stohastic gradient decent to lower the error after each epoch.\n",
    "#we have a class for each optimizer. one class for RMSpror, one for adam etc.\n",
    "#we did some expirement and the rmsprop was better for our model.\n",
    "#first argument -> all the parameters of our autoencoders. the parameters about the number of hidden layers\n",
    "#plus the amount of neurons each layer has and the activation function. Parameters that define the architecture of\n",
    "#our model\n",
    "#second argument ->is the learning rate. Experimental 0.01\n",
    "#third argument -> weight decay = used to reduce the learning rate after a few epochs and that's in order to regulate\n",
    "#the convergence. 0.5 based on experimenting, just like lr\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7gy59alAdloL"
   },
   "source": [
    "## Training the SAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FEz9hRaciFTs",
    "outputId": "0f6ed0d0-09c4-46c0-bfe6-70031d76b491"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1loss: tensor(0.8504)\n",
      "epoch: 2loss: tensor(0.8499)\n",
      "epoch: 3loss: tensor(0.8501)\n",
      "epoch: 4loss: tensor(0.8496)\n",
      "epoch: 5loss: tensor(0.8494)\n",
      "epoch: 6loss: tensor(0.8490)\n",
      "epoch: 7loss: tensor(0.8488)\n",
      "epoch: 8loss: tensor(0.8485)\n",
      "epoch: 9loss: tensor(0.8482)\n",
      "epoch: 10loss: tensor(0.8482)\n",
      "epoch: 11loss: tensor(0.8477)\n",
      "epoch: 12loss: tensor(0.8477)\n",
      "epoch: 13loss: tensor(0.8473)\n",
      "epoch: 14loss: tensor(0.8472)\n",
      "epoch: 15loss: tensor(0.8470)\n",
      "epoch: 16loss: tensor(0.8484)\n",
      "epoch: 17loss: tensor(0.8463)\n",
      "epoch: 18loss: tensor(0.8462)\n",
      "epoch: 19loss: tensor(0.8458)\n",
      "epoch: 20loss: tensor(0.8458)\n",
      "epoch: 21loss: tensor(0.8454)\n",
      "epoch: 22loss: tensor(0.8451)\n",
      "epoch: 23loss: tensor(0.8450)\n",
      "epoch: 24loss: tensor(0.8447)\n",
      "epoch: 25loss: tensor(0.8449)\n",
      "epoch: 26loss: tensor(0.8444)\n",
      "epoch: 27loss: tensor(0.8447)\n",
      "epoch: 28loss: tensor(0.8443)\n",
      "epoch: 29loss: tensor(0.8438)\n",
      "epoch: 30loss: tensor(0.8437)\n",
      "epoch: 31loss: tensor(0.8434)\n",
      "epoch: 32loss: tensor(0.8432)\n",
      "epoch: 33loss: tensor(0.8428)\n",
      "epoch: 34loss: tensor(0.8428)\n",
      "epoch: 35loss: tensor(0.8424)\n",
      "epoch: 36loss: tensor(0.8421)\n",
      "epoch: 37loss: tensor(0.8421)\n",
      "epoch: 38loss: tensor(0.8415)\n",
      "epoch: 39loss: tensor(0.8413)\n",
      "epoch: 40loss: tensor(0.8411)\n",
      "epoch: 41loss: tensor(0.8408)\n",
      "epoch: 42loss: tensor(0.8408)\n",
      "epoch: 43loss: tensor(0.8406)\n",
      "epoch: 44loss: tensor(0.8408)\n",
      "epoch: 45loss: tensor(0.8400)\n",
      "epoch: 46loss: tensor(0.8396)\n",
      "epoch: 47loss: tensor(0.8397)\n",
      "epoch: 48loss: tensor(0.8396)\n",
      "epoch: 49loss: tensor(0.8393)\n",
      "epoch: 50loss: tensor(0.8393)\n",
      "epoch: 51loss: tensor(0.8387)\n",
      "epoch: 52loss: tensor(0.8382)\n",
      "epoch: 53loss: tensor(0.8380)\n",
      "epoch: 54loss: tensor(0.8384)\n",
      "epoch: 55loss: tensor(0.8379)\n",
      "epoch: 56loss: tensor(0.8376)\n",
      "epoch: 57loss: tensor(0.8374)\n",
      "epoch: 58loss: tensor(0.8372)\n",
      "epoch: 59loss: tensor(0.8367)\n",
      "epoch: 60loss: tensor(0.8367)\n",
      "epoch: 61loss: tensor(0.8363)\n",
      "epoch: 62loss: tensor(0.8363)\n",
      "epoch: 63loss: tensor(0.8363)\n",
      "epoch: 64loss: tensor(0.8358)\n",
      "epoch: 65loss: tensor(0.8358)\n",
      "epoch: 66loss: tensor(0.8352)\n",
      "epoch: 67loss: tensor(0.8351)\n",
      "epoch: 68loss: tensor(0.8348)\n",
      "epoch: 69loss: tensor(0.8345)\n",
      "epoch: 70loss: tensor(0.8345)\n",
      "epoch: 71loss: tensor(0.8345)\n",
      "epoch: 72loss: tensor(0.8339)\n",
      "epoch: 73loss: tensor(0.8340)\n",
      "epoch: 74loss: tensor(0.8335)\n",
      "epoch: 75loss: tensor(0.8334)\n",
      "epoch: 76loss: tensor(0.8329)\n",
      "epoch: 77loss: tensor(0.8328)\n",
      "epoch: 78loss: tensor(0.8325)\n",
      "epoch: 79loss: tensor(0.8320)\n",
      "epoch: 80loss: tensor(0.8322)\n",
      "epoch: 81loss: tensor(0.8314)\n",
      "epoch: 82loss: tensor(0.8318)\n",
      "epoch: 83loss: tensor(0.8311)\n",
      "epoch: 84loss: tensor(0.8312)\n",
      "epoch: 85loss: tensor(0.8305)\n",
      "epoch: 86loss: tensor(0.8307)\n",
      "epoch: 87loss: tensor(0.8305)\n",
      "epoch: 88loss: tensor(0.8301)\n",
      "epoch: 89loss: tensor(0.8299)\n",
      "epoch: 90loss: tensor(0.8299)\n",
      "epoch: 91loss: tensor(0.8294)\n",
      "epoch: 92loss: tensor(0.8294)\n",
      "epoch: 93loss: tensor(0.8288)\n",
      "epoch: 94loss: tensor(0.8289)\n",
      "epoch: 95loss: tensor(0.8286)\n",
      "epoch: 96loss: tensor(0.8286)\n",
      "epoch: 97loss: tensor(0.8282)\n",
      "epoch: 98loss: tensor(0.8282)\n",
      "epoch: 99loss: tensor(0.8278)\n",
      "epoch: 100loss: tensor(0.8275)\n",
      "epoch: 101loss: tensor(0.8272)\n",
      "epoch: 102loss: tensor(0.8271)\n",
      "epoch: 103loss: tensor(0.8265)\n",
      "epoch: 104loss: tensor(0.8263)\n",
      "epoch: 105loss: tensor(0.8267)\n",
      "epoch: 106loss: tensor(0.8262)\n",
      "epoch: 107loss: tensor(0.8257)\n",
      "epoch: 108loss: tensor(0.8272)\n",
      "epoch: 109loss: tensor(0.8279)\n",
      "epoch: 110loss: tensor(0.8253)\n",
      "epoch: 111loss: tensor(0.8250)\n",
      "epoch: 112loss: tensor(0.8248)\n",
      "epoch: 113loss: tensor(0.8246)\n",
      "epoch: 114loss: tensor(0.8241)\n",
      "epoch: 115loss: tensor(0.8239)\n",
      "epoch: 116loss: tensor(0.8240)\n",
      "epoch: 117loss: tensor(0.8239)\n",
      "epoch: 118loss: tensor(0.8234)\n",
      "epoch: 119loss: tensor(0.8233)\n",
      "epoch: 120loss: tensor(0.8235)\n",
      "epoch: 121loss: tensor(0.8233)\n",
      "epoch: 122loss: tensor(0.8224)\n",
      "epoch: 123loss: tensor(0.8223)\n",
      "epoch: 124loss: tensor(0.8220)\n",
      "epoch: 125loss: tensor(0.8219)\n",
      "epoch: 126loss: tensor(0.8219)\n",
      "epoch: 127loss: tensor(0.8223)\n",
      "epoch: 128loss: tensor(0.8217)\n",
      "epoch: 129loss: tensor(0.8214)\n",
      "epoch: 130loss: tensor(0.8210)\n",
      "epoch: 131loss: tensor(0.8206)\n",
      "epoch: 132loss: tensor(0.8203)\n",
      "epoch: 133loss: tensor(0.8202)\n",
      "epoch: 134loss: tensor(0.8204)\n",
      "epoch: 135loss: tensor(0.8197)\n",
      "epoch: 136loss: tensor(0.8199)\n",
      "epoch: 137loss: tensor(0.8192)\n",
      "epoch: 138loss: tensor(0.8192)\n",
      "epoch: 139loss: tensor(0.8189)\n",
      "epoch: 140loss: tensor(0.8185)\n",
      "epoch: 141loss: tensor(0.8186)\n",
      "epoch: 142loss: tensor(0.8178)\n",
      "epoch: 143loss: tensor(0.8173)\n",
      "epoch: 144loss: tensor(0.8178)\n",
      "epoch: 145loss: tensor(0.8170)\n",
      "epoch: 146loss: tensor(0.8169)\n",
      "epoch: 147loss: tensor(0.8172)\n",
      "epoch: 148loss: tensor(0.8172)\n",
      "epoch: 149loss: tensor(0.8166)\n",
      "epoch: 150loss: tensor(0.8166)\n",
      "epoch: 151loss: tensor(0.8158)\n",
      "epoch: 152loss: tensor(0.8156)\n",
      "epoch: 153loss: tensor(0.8151)\n",
      "epoch: 154loss: tensor(0.8151)\n",
      "epoch: 155loss: tensor(0.8145)\n",
      "epoch: 156loss: tensor(0.8146)\n",
      "epoch: 157loss: tensor(0.8144)\n",
      "epoch: 158loss: tensor(0.8140)\n",
      "epoch: 159loss: tensor(0.8139)\n",
      "epoch: 160loss: tensor(0.8138)\n",
      "epoch: 161loss: tensor(0.8138)\n",
      "epoch: 162loss: tensor(0.8131)\n",
      "epoch: 163loss: tensor(0.8134)\n",
      "epoch: 164loss: tensor(0.8134)\n",
      "epoch: 165loss: tensor(0.8138)\n",
      "epoch: 166loss: tensor(0.8129)\n",
      "epoch: 167loss: tensor(0.8123)\n",
      "epoch: 168loss: tensor(0.8124)\n",
      "epoch: 169loss: tensor(0.8121)\n",
      "epoch: 170loss: tensor(0.8121)\n",
      "epoch: 171loss: tensor(0.8116)\n",
      "epoch: 172loss: tensor(0.8118)\n",
      "epoch: 173loss: tensor(0.8113)\n",
      "epoch: 174loss: tensor(0.8111)\n",
      "epoch: 175loss: tensor(0.8106)\n",
      "epoch: 176loss: tensor(0.8103)\n",
      "epoch: 177loss: tensor(0.8104)\n",
      "epoch: 178loss: tensor(0.8108)\n",
      "epoch: 179loss: tensor(0.8106)\n",
      "epoch: 180loss: tensor(0.8102)\n",
      "epoch: 181loss: tensor(0.8102)\n",
      "epoch: 182loss: tensor(0.8106)\n",
      "epoch: 183loss: tensor(0.8087)\n",
      "epoch: 184loss: tensor(0.8087)\n",
      "epoch: 185loss: tensor(0.8083)\n",
      "epoch: 186loss: tensor(0.8083)\n",
      "epoch: 187loss: tensor(0.8078)\n",
      "epoch: 188loss: tensor(0.8081)\n",
      "epoch: 189loss: tensor(0.8076)\n",
      "epoch: 190loss: tensor(0.8081)\n",
      "epoch: 191loss: tensor(0.8074)\n",
      "epoch: 192loss: tensor(0.8074)\n",
      "epoch: 193loss: tensor(0.8073)\n",
      "epoch: 194loss: tensor(0.8069)\n",
      "epoch: 195loss: tensor(0.8071)\n",
      "epoch: 196loss: tensor(0.8065)\n",
      "epoch: 197loss: tensor(0.8066)\n",
      "epoch: 198loss: tensor(0.8062)\n",
      "epoch: 199loss: tensor(0.8059)\n",
      "epoch: 200loss: tensor(0.8056)\n",
      "epoch: 201loss: tensor(0.8051)\n",
      "epoch: 202loss: tensor(0.8050)\n",
      "epoch: 203loss: tensor(0.8046)\n",
      "epoch: 204loss: tensor(0.8046)\n",
      "epoch: 205loss: tensor(0.8056)\n",
      "epoch: 206loss: tensor(0.8042)\n",
      "epoch: 207loss: tensor(0.8043)\n",
      "epoch: 208loss: tensor(0.8033)\n",
      "epoch: 209loss: tensor(0.8032)\n",
      "epoch: 210loss: tensor(0.8035)\n",
      "epoch: 211loss: tensor(0.8029)\n",
      "epoch: 212loss: tensor(0.8030)\n",
      "epoch: 213loss: tensor(0.8031)\n",
      "epoch: 214loss: tensor(0.8028)\n",
      "epoch: 215loss: tensor(0.8023)\n",
      "epoch: 216loss: tensor(0.8025)\n",
      "epoch: 217loss: tensor(0.8021)\n",
      "epoch: 218loss: tensor(0.8020)\n",
      "epoch: 219loss: tensor(0.8016)\n",
      "epoch: 220loss: tensor(0.8013)\n",
      "epoch: 221loss: tensor(0.8007)\n",
      "epoch: 222loss: tensor(0.8005)\n",
      "epoch: 223loss: tensor(0.8007)\n",
      "epoch: 224loss: tensor(0.8005)\n",
      "epoch: 225loss: tensor(0.8004)\n",
      "epoch: 226loss: tensor(0.8002)\n",
      "epoch: 227loss: tensor(0.7994)\n",
      "epoch: 228loss: tensor(0.7995)\n",
      "epoch: 229loss: tensor(0.7991)\n",
      "epoch: 230loss: tensor(0.7994)\n",
      "epoch: 231loss: tensor(0.7989)\n",
      "epoch: 232loss: tensor(0.7986)\n",
      "epoch: 233loss: tensor(0.7985)\n",
      "epoch: 234loss: tensor(0.7984)\n",
      "epoch: 235loss: tensor(0.7978)\n",
      "epoch: 236loss: tensor(0.7979)\n",
      "epoch: 237loss: tensor(0.7975)\n",
      "epoch: 238loss: tensor(0.7975)\n",
      "epoch: 239loss: tensor(0.7975)\n",
      "epoch: 240loss: tensor(0.7976)\n",
      "epoch: 241loss: tensor(0.7966)\n",
      "epoch: 242loss: tensor(0.7966)\n",
      "epoch: 243loss: tensor(0.7961)\n",
      "epoch: 244loss: tensor(0.7958)\n",
      "epoch: 245loss: tensor(0.7954)\n",
      "epoch: 246loss: tensor(0.7961)\n",
      "epoch: 247loss: tensor(0.7949)\n",
      "epoch: 248loss: tensor(0.7952)\n",
      "epoch: 249loss: tensor(0.7946)\n",
      "epoch: 250loss: tensor(0.7949)\n",
      "epoch: 251loss: tensor(0.7943)\n",
      "epoch: 252loss: tensor(0.7943)\n",
      "epoch: 253loss: tensor(0.7942)\n",
      "epoch: 254loss: tensor(0.7946)\n",
      "epoch: 255loss: tensor(0.7940)\n",
      "epoch: 256loss: tensor(0.7943)\n",
      "epoch: 257loss: tensor(0.7933)\n",
      "epoch: 258loss: tensor(0.7940)\n",
      "epoch: 259loss: tensor(0.7925)\n",
      "epoch: 260loss: tensor(0.7929)\n",
      "epoch: 261loss: tensor(0.7923)\n",
      "epoch: 262loss: tensor(0.7935)\n",
      "epoch: 263loss: tensor(0.7925)\n",
      "epoch: 264loss: tensor(0.7921)\n",
      "epoch: 265loss: tensor(0.7919)\n",
      "epoch: 266loss: tensor(0.7918)\n",
      "epoch: 267loss: tensor(0.7918)\n",
      "epoch: 268loss: tensor(0.7945)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 269loss: tensor(0.7920)\n",
      "epoch: 270loss: tensor(0.7911)\n",
      "epoch: 271loss: tensor(0.7911)\n",
      "epoch: 272loss: tensor(0.7915)\n",
      "epoch: 273loss: tensor(0.7905)\n",
      "epoch: 274loss: tensor(0.7904)\n",
      "epoch: 275loss: tensor(0.7900)\n",
      "epoch: 276loss: tensor(0.7901)\n",
      "epoch: 277loss: tensor(0.7899)\n",
      "epoch: 278loss: tensor(0.7903)\n",
      "epoch: 279loss: tensor(0.7897)\n",
      "epoch: 280loss: tensor(0.7903)\n",
      "epoch: 281loss: tensor(0.7888)\n",
      "epoch: 282loss: tensor(0.7895)\n",
      "epoch: 283loss: tensor(0.7889)\n",
      "epoch: 284loss: tensor(0.7889)\n",
      "epoch: 285loss: tensor(0.7882)\n",
      "epoch: 286loss: tensor(0.7888)\n",
      "epoch: 287loss: tensor(0.7896)\n",
      "epoch: 288loss: tensor(0.7884)\n",
      "epoch: 289loss: tensor(0.7876)\n",
      "epoch: 290loss: tensor(0.7875)\n",
      "epoch: 291loss: tensor(0.7871)\n",
      "epoch: 292loss: tensor(0.7873)\n",
      "epoch: 293loss: tensor(0.7868)\n",
      "epoch: 294loss: tensor(0.7863)\n",
      "epoch: 295loss: tensor(0.7863)\n",
      "epoch: 296loss: tensor(0.7866)\n",
      "epoch: 297loss: tensor(0.7863)\n",
      "epoch: 298loss: tensor(0.7856)\n",
      "epoch: 299loss: tensor(0.7853)\n",
      "epoch: 300loss: tensor(0.7853)\n",
      "epoch: 301loss: tensor(0.7851)\n",
      "epoch: 302loss: tensor(0.7850)\n",
      "epoch: 303loss: tensor(0.7847)\n",
      "epoch: 304loss: tensor(0.7850)\n",
      "epoch: 305loss: tensor(0.7843)\n",
      "epoch: 306loss: tensor(0.7844)\n",
      "epoch: 307loss: tensor(0.7842)\n",
      "epoch: 308loss: tensor(0.7844)\n",
      "epoch: 309loss: tensor(0.7837)\n",
      "epoch: 310loss: tensor(0.7843)\n",
      "epoch: 311loss: tensor(0.7840)\n",
      "epoch: 312loss: tensor(0.7833)\n",
      "epoch: 313loss: tensor(0.7832)\n",
      "epoch: 314loss: tensor(0.7827)\n",
      "epoch: 315loss: tensor(0.7820)\n",
      "epoch: 316loss: tensor(0.7824)\n",
      "epoch: 317loss: tensor(0.7819)\n",
      "epoch: 318loss: tensor(0.7820)\n",
      "epoch: 319loss: tensor(0.7817)\n",
      "epoch: 320loss: tensor(0.7817)\n",
      "epoch: 321loss: tensor(0.7813)\n",
      "epoch: 322loss: tensor(0.7814)\n",
      "epoch: 323loss: tensor(0.7816)\n",
      "epoch: 324loss: tensor(0.7814)\n",
      "epoch: 325loss: tensor(0.7814)\n",
      "epoch: 326loss: tensor(0.7807)\n",
      "epoch: 327loss: tensor(0.7806)\n",
      "epoch: 328loss: tensor(0.7805)\n",
      "epoch: 329loss: tensor(0.7802)\n",
      "epoch: 330loss: tensor(0.7809)\n",
      "epoch: 331loss: tensor(0.7800)\n",
      "epoch: 332loss: tensor(0.7805)\n",
      "epoch: 333loss: tensor(0.7797)\n",
      "epoch: 334loss: tensor(0.7798)\n",
      "epoch: 335loss: tensor(0.7795)\n",
      "epoch: 336loss: tensor(0.7791)\n",
      "epoch: 337loss: tensor(0.7784)\n",
      "epoch: 338loss: tensor(0.7788)\n",
      "epoch: 339loss: tensor(0.7779)\n",
      "epoch: 340loss: tensor(0.7778)\n",
      "epoch: 341loss: tensor(0.7779)\n",
      "epoch: 342loss: tensor(0.7785)\n",
      "epoch: 343loss: tensor(0.7784)\n",
      "epoch: 344loss: tensor(0.7776)\n",
      "epoch: 345loss: tensor(0.7774)\n",
      "epoch: 346loss: tensor(0.7776)\n",
      "epoch: 347loss: tensor(0.7771)\n",
      "epoch: 348loss: tensor(0.7769)\n",
      "epoch: 349loss: tensor(0.7772)\n",
      "epoch: 350loss: tensor(0.7768)\n",
      "epoch: 351loss: tensor(0.7763)\n",
      "epoch: 352loss: tensor(0.7762)\n",
      "epoch: 353loss: tensor(0.7761)\n",
      "epoch: 354loss: tensor(0.7759)\n",
      "epoch: 355loss: tensor(0.7759)\n",
      "epoch: 356loss: tensor(0.7891)\n",
      "epoch: 357loss: tensor(0.7870)\n",
      "epoch: 358loss: tensor(0.8057)\n",
      "epoch: 359loss: tensor(0.8239)\n",
      "epoch: 360loss: tensor(0.8165)\n",
      "epoch: 361loss: tensor(0.8091)\n",
      "epoch: 362loss: tensor(0.8045)\n",
      "epoch: 363loss: tensor(0.8031)\n",
      "epoch: 364loss: tensor(0.8042)\n",
      "epoch: 365loss: tensor(0.8061)\n",
      "epoch: 366loss: tensor(0.8019)\n",
      "epoch: 367loss: tensor(0.7970)\n",
      "epoch: 368loss: tensor(0.7941)\n",
      "epoch: 369loss: tensor(0.7926)\n",
      "epoch: 370loss: tensor(0.7922)\n",
      "epoch: 371loss: tensor(0.7909)\n",
      "epoch: 372loss: tensor(0.7892)\n",
      "epoch: 373loss: tensor(0.7905)\n",
      "epoch: 374loss: tensor(0.7889)\n",
      "epoch: 375loss: tensor(0.7875)\n",
      "epoch: 376loss: tensor(0.7864)\n",
      "epoch: 377loss: tensor(0.7881)\n",
      "epoch: 378loss: tensor(0.7866)\n",
      "epoch: 379loss: tensor(0.7857)\n",
      "epoch: 380loss: tensor(0.7850)\n",
      "epoch: 381loss: tensor(0.7853)\n",
      "epoch: 382loss: tensor(0.7843)\n",
      "epoch: 383loss: tensor(0.7844)\n",
      "epoch: 384loss: tensor(0.7845)\n",
      "epoch: 385loss: tensor(0.7845)\n",
      "epoch: 386loss: tensor(0.7831)\n",
      "epoch: 387loss: tensor(0.7825)\n",
      "epoch: 388loss: tensor(0.7817)\n",
      "epoch: 389loss: tensor(0.7828)\n",
      "epoch: 390loss: tensor(0.7809)\n",
      "epoch: 391loss: tensor(0.7813)\n",
      "epoch: 392loss: tensor(0.7810)\n",
      "epoch: 393loss: tensor(0.7803)\n",
      "epoch: 394loss: tensor(0.7807)\n",
      "epoch: 395loss: tensor(0.7801)\n",
      "epoch: 396loss: tensor(0.7787)\n",
      "epoch: 397loss: tensor(0.7788)\n",
      "epoch: 398loss: tensor(0.7780)\n",
      "epoch: 399loss: tensor(0.7778)\n",
      "epoch: 400loss: tensor(0.7772)\n",
      "epoch: 401loss: tensor(0.7769)\n",
      "epoch: 402loss: tensor(0.7775)\n",
      "epoch: 403loss: tensor(0.7772)\n",
      "epoch: 404loss: tensor(0.7767)\n",
      "epoch: 405loss: tensor(0.7814)\n",
      "epoch: 406loss: tensor(0.7921)\n",
      "epoch: 407loss: tensor(0.7799)\n",
      "epoch: 408loss: tensor(0.7792)\n",
      "epoch: 409loss: tensor(0.7766)\n",
      "epoch: 410loss: tensor(0.7760)\n",
      "epoch: 411loss: tensor(0.7753)\n",
      "epoch: 412loss: tensor(0.7756)\n",
      "epoch: 413loss: tensor(0.7761)\n",
      "epoch: 414loss: tensor(0.7748)\n",
      "epoch: 415loss: tensor(0.7751)\n",
      "epoch: 416loss: tensor(0.7744)\n",
      "epoch: 417loss: tensor(0.7743)\n",
      "epoch: 418loss: tensor(0.7734)\n",
      "epoch: 419loss: tensor(0.7740)\n",
      "epoch: 420loss: tensor(0.7733)\n",
      "epoch: 421loss: tensor(0.7734)\n",
      "epoch: 422loss: tensor(0.7740)\n",
      "epoch: 423loss: tensor(0.7736)\n",
      "epoch: 424loss: tensor(0.7736)\n",
      "epoch: 425loss: tensor(0.7730)\n",
      "epoch: 426loss: tensor(0.7729)\n",
      "epoch: 427loss: tensor(0.7720)\n",
      "epoch: 428loss: tensor(0.7717)\n",
      "epoch: 429loss: tensor(0.7722)\n",
      "epoch: 430loss: tensor(0.7723)\n",
      "epoch: 431loss: tensor(0.7717)\n",
      "epoch: 432loss: tensor(0.7715)\n",
      "epoch: 433loss: tensor(0.7707)\n",
      "epoch: 434loss: tensor(0.7710)\n",
      "epoch: 435loss: tensor(0.7708)\n",
      "epoch: 436loss: tensor(0.7707)\n",
      "epoch: 437loss: tensor(0.7706)\n",
      "epoch: 438loss: tensor(0.7700)\n",
      "epoch: 439loss: tensor(0.7706)\n",
      "epoch: 440loss: tensor(0.7702)\n",
      "epoch: 441loss: tensor(0.7701)\n",
      "epoch: 442loss: tensor(0.7694)\n",
      "epoch: 443loss: tensor(0.7695)\n",
      "epoch: 444loss: tensor(0.7691)\n",
      "epoch: 445loss: tensor(0.7685)\n",
      "epoch: 446loss: tensor(0.7686)\n",
      "epoch: 447loss: tensor(0.7680)\n",
      "epoch: 448loss: tensor(0.7674)\n",
      "epoch: 449loss: tensor(0.7671)\n",
      "epoch: 450loss: tensor(0.7674)\n",
      "epoch: 451loss: tensor(0.7667)\n",
      "epoch: 452loss: tensor(0.7671)\n",
      "epoch: 453loss: tensor(0.7699)\n",
      "epoch: 454loss: tensor(0.7678)\n",
      "epoch: 455loss: tensor(0.7675)\n",
      "epoch: 456loss: tensor(0.7668)\n",
      "epoch: 457loss: tensor(0.7670)\n",
      "epoch: 458loss: tensor(0.7663)\n",
      "epoch: 459loss: tensor(0.7666)\n",
      "epoch: 460loss: tensor(0.7661)\n",
      "epoch: 461loss: tensor(0.7661)\n",
      "epoch: 462loss: tensor(0.7664)\n",
      "epoch: 463loss: tensor(0.7662)\n",
      "epoch: 464loss: tensor(0.7652)\n",
      "epoch: 465loss: tensor(0.7648)\n",
      "epoch: 466loss: tensor(0.7650)\n",
      "epoch: 467loss: tensor(0.7646)\n",
      "epoch: 468loss: tensor(0.7638)\n",
      "epoch: 469loss: tensor(0.7639)\n",
      "epoch: 470loss: tensor(0.7642)\n",
      "epoch: 471loss: tensor(0.7638)\n",
      "epoch: 472loss: tensor(0.7641)\n",
      "epoch: 473loss: tensor(0.7658)\n",
      "epoch: 474loss: tensor(0.7639)\n",
      "epoch: 475loss: tensor(0.7640)\n",
      "epoch: 476loss: tensor(0.7642)\n",
      "epoch: 477loss: tensor(0.7636)\n",
      "epoch: 478loss: tensor(0.7628)\n",
      "epoch: 479loss: tensor(0.7630)\n",
      "epoch: 480loss: tensor(0.7622)\n",
      "epoch: 481loss: tensor(0.7625)\n",
      "epoch: 482loss: tensor(0.7622)\n",
      "epoch: 483loss: tensor(0.7616)\n",
      "epoch: 484loss: tensor(0.7612)\n",
      "epoch: 485loss: tensor(0.7616)\n",
      "epoch: 486loss: tensor(0.7612)\n",
      "epoch: 487loss: tensor(0.7614)\n",
      "epoch: 488loss: tensor(0.7613)\n",
      "epoch: 489loss: tensor(0.7610)\n",
      "epoch: 490loss: tensor(0.7609)\n",
      "epoch: 491loss: tensor(0.7609)\n",
      "epoch: 492loss: tensor(0.7603)\n",
      "epoch: 493loss: tensor(0.7612)\n",
      "epoch: 494loss: tensor(0.7605)\n",
      "epoch: 495loss: tensor(0.7609)\n",
      "epoch: 496loss: tensor(0.7600)\n",
      "epoch: 497loss: tensor(0.7604)\n",
      "epoch: 498loss: tensor(0.7598)\n",
      "epoch: 499loss: tensor(0.7601)\n",
      "epoch: 500loss: tensor(0.7606)\n"
     ]
    }
   ],
   "source": [
    "#we need optimized code that saves the memory if we want to do a project with a lot of ratings etc\n",
    "#define our number of epoch. Weights updated after epoch. Number is based on expirementing\n",
    "nb_epoch = 200\n",
    "#in each epoch we will loop into our all users and the ratings each user has.\n",
    "for epoch in range(1, nb_epoch + 1): #+1 because upper bound is excluded.\n",
    "  #init our train loss error\n",
    "  train_loss = 0\n",
    "  #init a counter. it will count the number of users that rated at least one movie.\n",
    "  #we don't want to computate for users that didn't give rating for any movie. 0. = float.\n",
    "  s = 0.\n",
    "  for id_user in range(nb_users): #we just want the indexes which start from 0 to 942 so we are ok with just nb_users\n",
    "    #for each user. we get the input vector that contains all the ratings \n",
    "    #take our training_set[id_user]. Pytorch cannot accept a single vector but only a batch. The functions like forward\n",
    "    #cannot take a single vector of one dimension. We need to add a fake dimension like in keras which will correspond\n",
    "    #to the batch. Pytorch tecnique. Variable function with imput training_set[id_user].\n",
    "    #. unsqueeze(index of the new dimension). means where do we want the new dimension? se poia thesi? first index\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    #what about the target? We need to do the same. We will modify the origiran input and since we want \n",
    "    #the original input before the modification we will create target var. clone -> copy of input.\n",
    "    target = input.clone()\n",
    "    #MEMORY OPTIMIZATION : we won't care if our user who didn't rate any movie.\n",
    "    #take all the values of target which is the input vector and sum them up(all the ratings) larget than 0\n",
    "    #check if the sums of 1,2,3,4,5 is larger than 0. If it is then it contains at least one rating\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        #get the ouptut vector of predicted ratings, by applying the sae in our input vector.\n",
    "      output = sae(input)\n",
    "        #we want to make sure the gradient is calculated only with respect to the input and not the target\n",
    "        #reduce the computation. Require_grad = this will make sure we don't compute the gradient with respect \n",
    "        #to the target. That optimizes our code.\n",
    "      target.require_grad = False\n",
    "        #another optimization. in the future computations of our stohasstic gradient descent we only want to include in\n",
    "        #the computation the non zero values. We don;t want to deal with the movies the user didn't rate.\n",
    "        #only for the output vector.\n",
    "        #take the values of the output such as target == 0 and zero them.\n",
    "        #these values will not count in the computation of the error and will not have an impact after we measure\n",
    "        #the error because they will not count in the rsmprop computation.\n",
    "      output[target == 0] = 0\n",
    "        #loss error. \n",
    "      loss = criterion(output, target)\n",
    "      mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "      loss.backward()\n",
    "      train_loss += np.sqrt(loss.data*mean_corrector)\n",
    "      s += 1.\n",
    "      optimizer.step()\n",
    "  print('epoch: '+str(epoch)+'loss: '+ str(train_loss/s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bak5uc8gd-gX"
   },
   "source": [
    "## Testing the SAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5ztvzYRtiGCz",
    "outputId": "d0e8ea8b-9ac4-40e5-a19a-7fcfc6934d61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: tensor(0.9681)\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "s = 0.\n",
    "for id_user in range(nb_users):\n",
    "  input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "  target = Variable(test_set[id_user]).unsqueeze(0)\n",
    "  if torch.sum(target.data > 0) > 0:\n",
    "    output = sae(input)\n",
    "    target.require_grad = False\n",
    "    output[target == 0] = 0\n",
    "    loss = criterion(output, target)\n",
    "    mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "    test_loss += np.sqrt(loss.data*mean_corrector)\n",
    "    s += 1.\n",
    "print('test loss: '+str(test_loss/s))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AutoEncoders.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
